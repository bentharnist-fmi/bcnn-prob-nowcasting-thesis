\chapter{Background}
\label{chapter:background} 
\fixme{Estimate at start: 12-15 pages}

\section{Precipitation Nowcasting}

\subsection{Weather radars and radar products}


\begin{itemize}
	\item physical functioning and paragraph on history of weather radar
	\item reflectivity products, 2D and 3D
	
	\item from reflectivity to RR, limitations of reflectivity
	\item double-polarization weather radars and Polarimetric products: info on microphysical processes
\end{itemize}

\begin{equation}
\label{eq:z-r}
	z = AR^b
\end{equation}

where $z$ is reflectivity, $R$ is rainrate, and $A$ as well as $b$ are empirically determined parameters. Reflectivity is calculated 

\subsection{Overview of weather forecast methods}
\begin{itemize}
	\item Taking a broad look into the NWP process: Data collection, data assimilation, numerical prediction, postprocessing into forecast products
	\item NWP in the context of precipitation forecasts
\end{itemize}


\subsection{Precipitation nowcasting : classical deterministic methods}
\label{section:classic_nowcast}
\begin{itemize}
	\item Basic principles of advection equations. 
	\item Chronological advancing, with +/- of each method
\end{itemize}

\subsection{Precipitation nowcasting : classical probabilistic methods}
\begin{itemize}
	\item approach types, ensemble etc
\end{itemize}

\subsection{Machine learning approaches to precipitation nowcasting}
\begin{itemize}
	\item 
\end{itemize}

Multi-Scale Structural Similarity Index (MS-SSIM), originally an image quality assessment metric \cite{wang_multiscale_2003}, has recently started to be used as a loss function in deep learning, particularly in image reconstruction tasks. One of its main assets is that it or its single-scale counterpart (SSIM )has been shown to reduce blurring in image reconstruction \cite{zhao_loss_2017} making it a good candidate loss function for nowcasting with neural networks, as it has been seen that these models suffer from excessive blurring, effectively minimizing standard loss functions such as Mean Squared Error (MSE), but failing to predict useful patterns such as heavy localized rainfall. Indeed in recent years, there has been some cases where MS-SSIM or SSIM started have started to be used in 

\section{Bayesian deep learning}

\subsection{Learning probability distributions}
\begin{itemize}
	\item first old studies
	\item link overfitting to the bias-variance trade-off
\end{itemize}

	Neural networks are extremely useful model that on the other hand are very complex, in the sense that they have many optimizable parameters. The effect of this is that they are sensible to \textit{overfitting}, meaning that if left unchecked they will learn spurious patterns in the data, over-adapting to the training dataset and worsening generalization ability. In order to counter this, different mechanisms exist that bias the neural network towards learning simpler representations that have better generalization ability when presented with out-of-training-data examples. This concept is known as \textit{regularization}. Over the past decades, many regularization mechanisms have been successfully developed and applied to the training of deep neural networks. Notable examples include Dropout and weight decay, also known as L2-regularization.  
	
	One caveat of these classical regularization methods is that they do not enable representing the uncertainty of neural networks in unexplored regions, although they do improve performance in them. As it has understandably many benefits to make a neural network able to say "I don't know", a way to represent different plausible scenarios arising with novel data is needed. 
	
	There are two ways of approaching uncertainty estimation in neural networks. One is to directly model the uncertainty of predictions, and the other is to model the uncertainty of model parameters. These two are often complementary, but The latter approach has the benefit of providing implicit regularization to the network and is indeed the primary focus of this work. Learning this uncertainty of model parameters is most often accomplished using Bayesian Neural Networks (BNN). Strictly speaking, BNN are a class of stochastic neural networks, that is NN with stochastic components, where the parameters are probability distributions that are estimated using Bayesian inference.
	
	In a Bayesian framework, the posterior distributions of parameters are estimated conditioned on data and a prior. Because exact Bayesian inference involves dealing with intractable integrals, it is not doable to solve them for real-world neural networks having thousands to millions of parameters. As such, two broad categories of inference methods have been developed for use in BNN. The first one is using Markov-Chain Monte-Carlo (MCMC) to sample from posterior distributions. MCMC works well for smaller-scale models, but it is limited to only thousands of parameters because of the computational expensiveness of Markov-Chain simulations. 
	
	The second inference method category is Variational Inference (VI). The main idea behind variational inference is to turn the problem of finding the intractable true bayesian posterior into that of finding the closest distribution from a limited distribution family (the variational posterior) with regards to the true posterior. This turns the problem of solving an integral into that of optimization, allowing the use of classical unconstrained optimization methods. 
	
	

\subsection{Variational inference}
\label{section:vi}


	Variational inference as a means of training bayesian neural networks was first introduced by Hinton and Camp in 1993 \cite{hinton_keeping_1993}. However it was not used for a long time before breakthroughs by Graves et al. (2011) \cite{graves_practical_2011} and Blundell et al. (2015) \cite{blundell_weight_2015} permitted its use in large-scale neural networks. 
	
	 

	\begin{itemize}
		\item deriving ELBO
		
		
		\item Bayes by Backprop and SVI
		\item Local reparametrization
		\item Monte-Carlo dropout as VI
		\item where else is VI used
		\item problems with VI
	\end{itemize}

	The evidence lower bound (ELBO), also known as variational free energy, is thus defined as 
	
	\begin{equation}
	\label{eq:elbo}
		\mathcal{F}(\mathcal{D}, \sigma) = 
		\text{KL}[q(\pmb{w}|\sigma) || P(\pmb{w})] - \mathbb{E}_{q(\pmb{w}|\sigma)}[\log(P(\mathcal{D}|\pmb{w}))]
	\end{equation}

	In order to apply backpropagation, the standard deviation is parametrized with a parameter $\rho \in \mathbb{R}$, such that $\sigma = \log(1 + \exp(\rho)) \in (0,\infty)$. 
	
	This is known as the reparametrization trick. Weights $\pmb{w} = t(\sigma, \epsilon)$ are made deterministic functions of posterior parameters and the external stochastic noise $\epsilon$. This allows gradients to flow through the network as sampling operations are external. 
	
	\begin{itemize}
		\item how does variance scale for basic re-parametrization trick?
		\item more Details on LRT
	\end{itemize}

	Kingma et al. (2015) \cite{kingma_variational_2015} introduced a modification to the reparametrization trick, called the \textit{local reparametrization trick} (LRT). It works similarly but layer activations are sampled using $\epsilon$ rather than weights in order to reduce the computational cost, and even more importantly variances in a minibatch, allowing for faster training. 
	
	\begin{itemize}
		\item Flipout and its possible advantages over LRT
	\end{itemize}
	
	Using the reparametrization trick and 
	
	\begin{equation}
	\frac{\partial}{\partial \sigma} \mathbb{E}_{q(\pmb{w}|\sigma)}[f(\pmb{w}, \sigma)] =
	\mathbb{E}_{q(\epsilon)}[\frac{\partial f(\pmb{w}, \sigma)}{\partial \pmb{w}} + \frac{\partial \pmb{w}}{\partial \sigma}
	\frac{\partial f(\pmb{w}, \sigma)}{\partial \pmb{w}}]
	\end{equation}
	
	the ELBO cost function (Eq. \ref{eq:elbo}) can be rewritten as in a form amendable to minibatch optimization, yielding
	
	\begin{itemize}
		\item Elaborate on the above proposition by Blundell et al
	\end{itemize}
	
	\begin{equation}
	\label{eq:mini_elbo}
	\mathcal{F}(\mathcal{D}, \sigma) \approx \sum_{i=1}^{n}\log q(\pmb{w}^{(i)}|\sigma) - \log P(\pmb{w}^{(i)}) - \log P(\mathcal{D}|\pmb{w}^{(i)})
	\end{equation}
	
	Here $\pmb{w}^{(i)}$ denotes the $i$:th monte-carlo sample drawn the the posterior.
	
	From the terms in eq. \ref{eq:mini_elbo}:
	\begin{enumerate}
		\item $\log q(\pmb{w}^{(i)}|\sigma)$ is the log likelihood of weights given the current posterior distribution.
		
		\item $-\log P(\pmb{w}^{(i)})$ is the negative log likelihood of the weights given the prior, which is usually not learned and serves as a regularization mechanism. 
		
		\item $- \log P(\mathcal{D}|\pmb{w}^{(i)})$ is the likelihood term, dependent on the data $\mathcal{D}$
	\end{enumerate}
	
	\vspace*{2mm}
	The first two terms are grouped together as the complexity cost, while the last term is also often called the complexity cost. 
	
	The cost to be minimizing can be again rewritten as 
	
	\begin{equation}
		\mathcal{F}^\pi_i(\mathcal{D_i}, \sigma) = \pi_i \text{KL}[q(\pmb{w}|\sigma) || P(\pmb{w})] - \mathbb{E}_{q(\pmb{w}|\sigma)}[\log P(\mathcal{D}_i|\pmb{w})]
	\end{equation}
	
	Here $\pi_i$ is the relative weighting of the complexity cost in the $i$:th minibatch. 
	There are many ways to weigh the complexity cost against the likelihood cost, but an usual constraint is that $\pi \in [0,1]^M$ and $\sum_{i=1}^{M} \pi_i = 1$. Graves et al. (2011) \cite{graves_practical_2011} used equal weighting as $\pi_i = 1/M$, but Blundell et al. (2015) \cite{blundell_weight_2015} found the scheme $\pi_i =\frac{2^{M-i}}{2^M - 1}$ to offer better performance in their experiments. 

	
	The prior distribution $P(\pmb{w})$ is often chosen to be a diagonal Gaussian distribution, because it allows calculating the KL-divergence with regards to the Gaussian posterior analytically. A Gaussian prior placed on weights would be equivalent to L2-regularization, also known as weight decay. Recently, despite of the inability to analytically calculate KL-divergences using it, Gaussian scale mixture priors have also started to be used \cite{blundell_weight_2015, shridhar_comprehensive_2019} because of their properties facilitating optimization. These scale mixture priors, if containing two scales are defined as 
	
	\begin{equation}
	\label{eq:gsm}
		P(\pmb{w}) = 
		\prod_{j}\alpha \mathcal{N}(\pmb{w}_j|0,\sigma_1^2) + (1-\alpha)\mathcal{N}(\pmb{w}_j|0,\sigma_2^2)
	\end{equation}  
	
	where $\sigma_1 > \sigma_2$ and often $\sigma_2 \ll 1$, $\alpha$ is the relative weights of the two scales, and $\pmb{w}_j$ is the $j$:th weight of the neural network. 
	
\subsection{Predictive uncertainty estimation and decomposition}

\fixme{ find good ref on sources}
\begin{itemize}
	\item Where does uncertainty come from
	\item division into epistemic, aleatoric uncertainty in ML
	\item Division for classification
	\item Division for regression 
	
\end{itemize}

\section{Related work}

\fixme{Section to include if 2.1.4 and 2.1.5 become convoluted}

