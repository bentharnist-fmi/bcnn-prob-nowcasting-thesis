
@article{shridhar_comprehensive_2019,
	title = {A {Comprehensive} guide to {Bayesian} {Convolutional} {Neural} {Network} with {Variational} {Inference}},
	url = {http://arxiv.org/abs/1901.02731},
	abstract = {Artificial Neural Networks are connectionist systems that perform a given task by learning on examples without having prior knowledge about the task. This is done by finding an optimal point estimate for the weights in every node. Generally, the network using point estimates as weights perform well with large datasets, but they fail to express uncertainty in regions with little or no data, leading to overconfident decisions. In this paper, Bayesian Convolutional Neural Network (BayesCNN) using Variational Inference is proposed, that introduces probability distribution over the weights. Furthermore, the proposed BayesCNN architecture is applied to tasks like Image Classification, Image Super-Resolution and Generative Adversarial Networks. The results are compared to point-estimates based architectures on MNIST, CIFAR-10 and CIFAR-100 datasets for Image CLassification task, on BSD300 dataset for Image Super Resolution task and on CIFAR10 dataset again for Generative Adversarial Network task. BayesCNN is based on Bayes by Backprop which derives a variational approximation to the true posterior. We, therefore, introduce the idea of applying two convolutional operations, one for the mean and one for the variance. Our proposed method not only achieves performances equivalent to frequentist inference in identical architectures but also incorporate a measurement for uncertainties and regularisation. It further eliminates the use of dropout in the model. Moreover, we predict how certain the model prediction is based on the epistemic and aleatoric uncertainties and empirically show how the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the training accuracy increases. Finally, we propose ways to prune the Bayesian architecture and to make it more computational and time effective.},
	urldate = {2022-03-18},
	journal = {arXiv:1901.02731 [cs, stat]},
	author = {Shridhar, Kumar and Laumann, Felix and Liwicki, Marcus},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.02731},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/harnist/Zotero/storage/LS296KMN/Shridhar et al. - 2019 - A Comprehensive guide to Bayesian Convolutional Ne.pdf:application/pdf;arXiv.org Snapshot:/home/harnist/Zotero/storage/XVEFISX3/1901.html:text/html},
}

@article{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efﬁcient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classiﬁcation. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	language = {en},
	urldate = {2022-03-21},
	journal = {arXiv:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:/home/harnist/Zotero/storage/3UGR5FY5/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf},
}

@article{franch_taasrad19_2020,
	title = {{TAASRAD19}, a high-resolution weather radar reflectivity dataset for precipitation nowcasting},
	volume = {7},
	copyright = {2020 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-020-0574-8},
	doi = {10.1038/s41597-020-0574-8},
	abstract = {We introduce TAASRAD19, a high-resolution radar reflectivity dataset collected by the Civil Protection weather radar of the Trentino South Tyrol Region, in the Italian Alps. The dataset includes 894,916 timesteps of precipitation from more than 9 years of data, offering a novel resource to develop and benchmark analog ensemble models and machine learning solutions for precipitation nowcasting. Data are expressed as 2D images, considering the maximum reflectivity on the vertical section at 5 min sampling rate, covering an area of 240 km of diameter at 500 m horizontal resolution. The TAASRAD19 distribution also includes a curated set of 1,732 sequences, for a total of 362,233 radar images, labeled with precipitation type tags assigned by expert meteorologists. We validate TAASRAD19 as a benchmark for nowcasting methods by introducing a TrajGRU deep learning model to forecast reflectivity, and a procedure based on the UMAP dimensionality reduction algorithm for interactive exploration. Software methods for data pre-processing, model training and inference, and a pre-trained model are publicly available on GitHub (https://github.com/MPBA/TAASRAD19) for study replication and reproducibility.},
	language = {en},
	number = {1},
	urldate = {2022-04-07},
	journal = {Scientific Data},
	author = {Franch, Gabriele and Maggio, Valerio and Coviello, Luca and Pendesini, Marta and Jurman, Giuseppe and Furlanello, Cesare},
	month = jul,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Atmospheric dynamics},
	pages = {234},
	file = {Full Text PDF:/home/harnist/Zotero/storage/7KASMA3J/Franch et al. - 2020 - TAASRAD19, a high-resolution weather radar reflect.pdf:application/pdf;Snapshot:/home/harnist/Zotero/storage/J6HJN5T6/s41597-020-0574-8.html:text/html},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2022-04-17},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	pages = {859--877},
	file = {arXiv Fulltext PDF:/home/harnist/Zotero/storage/MD5BZCQ5/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:/home/harnist/Zotero/storage/BG9BRKM9/1601.html:text/html},
}

@misc{noauthor_contents_nodate,
	title = {Contents},
	url = {https://ermongroup.github.io/cs228-notes/},
	urldate = {2022-04-17},
	file = {Contents:/home/harnist/Zotero/storage/6ZSGHD7A/cs228-notes.html:text/html},
}

@misc{noauthor_high-level_nodate,
	title = {High-{Level} {Explanation} of {Variational} {Inference}},
	url = {https://www.cs.jhu.edu/~jason/tutorials/variational.html},
	urldate = {2022-04-17},
	file = {High-Level Explanation of Variational Inference:/home/harnist/Zotero/storage/UNTJVUD4/variational.html:text/html},
}

@article{jospin_hands-bayesian_2022,
	title = {Hands-on {Bayesian} {Neural} {Networks} -- a {Tutorial} for {Deep} {Learning} {Users}},
	url = {http://arxiv.org/abs/2007.06823},
	abstract = {Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides deep learning practitioners with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks, i.e., stochastic artiﬁcial neural networks trained using Bayesian methods.},
	language = {en},
	urldate = {2022-04-18},
	journal = {arXiv:2007.06823 [cs, stat]},
	author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
	month = jan,
	year = {2022},
	note = {arXiv: 2007.06823},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 62-02 (Primary), G.3, I.2.6},
	file = {Jospin et al. - 2022 - Hands-on Bayesian Neural Networks -- a Tutorial fo.pdf:/home/harnist/Zotero/storage/KVRYW8KT/Jospin et al. - 2022 - Hands-on Bayesian Neural Networks -- a Tutorial fo.pdf:application/pdf},
}
